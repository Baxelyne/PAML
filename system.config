### use # to comment out the configure item

################ Status ################
mode=train
# string: train/test

################ Datasets(Input/Output) ################
datasets_fold=data/dataset
train_file=train.data
dev_file=dev.data
test_file=test.data

delimiter=t
# string: (t: "\t";"table")|(b: "backspace";" ")|(other, e.g., '|||', ...)

use_pretrained_embedding=False
token_emb_dir=data/dataset/word.emb

vocabs_dir=data/dataset/vocabs

log_dir=data/dataset/logs

checkpoints_dir=checkpoints/datasets

################ Labeling Scheme ################
label_scheme=BIO
# string: BIO/BIESO

label_level=2
# int, 1:BIO/BIESO; 2:BIO/BIESO + suffix
# max to 2

hyphen=-
# string: -|_, for connecting the prefix and suffix: `B_PER', `I_LOC'

suffix=[DNA,protein,cell_type]
# unnecessary if label_level=1

labeling_level=word
# string: word/char
# for English: （word: hello），（char: h）
# for Chinese: （word: 你好），（char: 你）

measuring_metrics=[precision,recall,f1,accuracy]
# string: accuracy|precision|recall|f1
# f1 is compulsory
################ Model Configuration ################
use_crf=True

cell_type=LSTM
# LSTM, GRU
biderectional=True
encoder_layers=1

embedding_dim=100
#int, must be consistent with `token_emb_dir' file

hidden_dim=100

position_size=300
position_dim=50

max_sequence_length=100

use_self_attention=False
attention_dim=500

seed=42

################ Training Settings ###
epoch=300
batch_size=3

dropout=0.01
learning_rate=0.0005

optimizer=Adam
#string: GD/Adagrad/AdaDelta/RMSprop/Adam

checkpoints_max_to_keep=1
print_per_batch=1

is_early_stop=True
patient=10

checkpoint_name=model

################ Testing Settings ###
output_test_file=test.out

is_output_sentence_entity=True
output_sentence_entity_file=test.entity.out

